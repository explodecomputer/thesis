\chapter{Discussion}
\label{Discussion}
\lhead{Chapter 5. \emph{Discussion}}


\section{Objectives}

After being predicated on a largely additive statistical paradigm, the disappointing results of large scale genome wide association studies over the last decade have caused the architecture of natural genetic variance to come into question \citep{Eichler2010}. Epistasis has an established place in population and quantitative genetic theory, yet since the advent of GWAS it has been largely neglected from empirical exploration. There have been three major reasons for this. Firstly, estimates of genetic components have frequently suggested that there is little statistical contribution from non-additive effects \citep{Hill2008a}. Secondly, with the acceleration in genotyping technology the computational barrier of searching for genetic interactions soon became insurmountable with standard programming techniques. And thirdly, while the statistical power to detect small independent effects is already low, the problem becomes considerably more acute when extending the search to interactions.

This thesis has attempted to address each of these issues. To briefly summarise, it was shown that the maintenance of additive variation in fitness related traits can be achieved through epistatic interactions, and that the presence of additive variance may in fact be symptomatic of a more complex genomic architecture. The computational challenges of searching for these interactions exhaustively were mitigated through the use of an emerging form of parallel programming based on cheap, consumer level graphics cards. Indeed the performance of these devices is such that if using CPUs, the permutation experiments performed in chapter 4 will have taken up to 200 compute years, but with the availability of modestly sized GPGPU clusters this was completed in a few months of user time. Finally, it was attempted to address some of the statistical challenges by parameterising SNP data as haplotypes in order to rescue the LD with unobserved causal variants, and through extensive simulations it can be shown that when combined with penalised regression techniques this can effect a significant improvement in statistical power.


\section{Further considerations}

\subsection{Higher order interactions}

The work presented here considers epistasis only from a fairly narrow perspective - that of two locus interactions. The computational challenge of searching exhaustively in 3 dimensions or more is currently insurmountable with the density of marker information required to adequately capture the variance of untyped causal variants, and perhaps more to the point the tractability of exhaustive searching from a statistical point of view is questionable, both in terms of the significance thresholds required and the potential combinatorial problems that were discussed in chapter 4.

It may be the case that searching in higher than two dimensions is not necessary. For example, the discovery of epistatic networks involving many loci can be made from two dimensional searches alone (\emph{e.g.} \citet{Carlborg2006}), and just as independent SNPs can explain some of the variance of their joint effects, it would be interesting to explore how much of the variance of higher order interactions can be explained marginally in two dimensions.

The evolutionary properties of higher order interactions are also difficult to predict, in particular it may be the case that higher order interactions maintain additive variance more effectively than the two locus interactions explored in chapter 2. Should this be the case then it may be of practical importance to understand how best to map such genetic factors. One potential approach could be to test for variance heterogeneity within loci. While it has been shown that this is not as powerful for detecting two locus interactions as the direct method of treating them as fixed effects \citep{Struchalin2010}, the behaviour with regards to higher order interactions is unknown. 


\subsection{Phenomics}

The number of genetic factors, or the polygenicity, governing heritable traits is fairly variable. For example, susceptibility to infectious disease is speculated to be under the control of relatively few polymorphisms with large effects \citep{Min-Oo2003, Diez2003}, while other traits such as height, obesity, and red blood cell count appear to be much more in the dominion of the infinitesimal model \citep{Valdar2006, Park2010}, being controlled by very many variants of small effects. One approach that is used to overcome the problem of high polygenicity is to expand the sample size of the study. Perhaps the most extreme example of this to date is the meta-analysis of human height performed by \citet{LangoAllen2010} which included over 180000 individuals in total. While this was effective at detecting many variants ($>180$), the proportion of phenotypic variance explained remained fairly low at around 10\% overall.

To generalise, it could be said that the set of predictor variables in a GWAS, the fixed genetic effects, are extremely well characterised, while the response variable is rarely anything more than a single binary or quantitative variable. Perhaps a more effective way to overcome high polygenicity would be to redress this balance through the inclusion of large-scale phenotyping (``phenomics", \citet{Sabb2009, Houle2010}). The argument for this approach is that one reason that high level disease phenotypes are so polygenic is that they are the manifestation of many different lower level phenotypes. If these lower level phenotypes are less polygenic then mapping genetic variation to them may be more statistically tractable. Thus an overall understanding of the genetic components contributing to high level disease trait of interest could then be composed by reconstructing the relationships between lower level phenotypes.

Some success has already been achieved with this type of approach. For example expression QTL (eQTL) studies that aim to map genetic variation to variation in gene expression levels, arguably a very low level phenotype, tend to uncover extremely large effect sizes relative to the higher level morphological phenotypes that are more commonly used in GWAS. Resultantly, high proportions of the variance of expression can be detected with relatively modest sample sizes \citep{Bystrykh2005, Cookson2009}. Typically these types of studies are restricted to searching for additive \emph{cis}-QTLs and it is uncommon to extend the analysis to include genome-wide epistasis, thus there is potential for these types of studies to expand in scope and begin to assay the underlying architecture of genetic variation.

While the use of lower level phenotypes has the potential to overcome the problems associated with the infinitesimal model, employing such an approach in an integrative phenomics model may have its own set of problems. High throughput phenotyping is currently possible for gene expression, proteomic and metabolic data, but the question of implementation is still difficult. For example, a realistic assay of the phenome might involve the collection of this type of data from multiple tissues at several time points. Aside from being potentially invasive, this is most likely financially prohibitive. In addition, from a statistical point of view the problem of causality emerges. When constructing relationships between low level phenotypes, assigning directionality to the effects in a network is difficult \citep{Shipley2000}, and this may be very limiting for prediction accuracy. For example which low level phenotypes are upstream of the manifestation of disease (causative), which are downstream (consequential), and which are simply confounded? Theoretically it is only the upstream events that will have predictive value for the outcome of disease phenotypes \citep{Shipley2000} and uncovering correlations alone in the phenome will be insufficient for a realistic prediction model.


\subsection{Threshold based searches}

Though the broad goal is to identify causal variants, GWAS in its standard form is generally only concerned with identifying regions of association. The question of the direction of causality in genetic associations is undisputed, logically phenotypes cannot `cause' a genetic variant. This is an uncommon feature in data mining in general, the inference of causality often being difficult to ascertain in most statistical frameworks. However the belief that the variant has a real biological association is disputable, even after surpassing the stringent family-wise testing thresholds that are routinely employed. Indeed, the philosophical question of what level of evidence is sufficient in order to be confident that a variant has a true biological effect is difficult to answer. But it should be clear that in the context of complex traits, where many effects contribute to the genetic variance, and each effect is supposedly small, statistical association in a single study alone is insufficient.

One approach to validating a candidate signal is to search for the same effect in an independent population and this is often a requirement for publication, although commonly the threshold for replication is relaxed. For example, \citet{Siontis2010} showed that in a survey of 291 candidate SNPs, only 41 were replicated with $p$-value $< 10^{-7}$, and the Catalog of Published Genome-Wide Association Studies \citep{Hindorff2010}, comprises 5845 significant associations, of which 2589 have not demonstrated any replication, and the median association $p$-value of ($1\times10^{-7}$) is below the widely suggested comparison-wise threshold of $7.2\times10^{-8}$ \citep{Dudbridge2008}. Even more extreme, \citet{Hirschhorn2002} surveyed 166 variants that had been studied in three populations, and found that only 6 were consistently replicated. Indeed \citet{Liu2008} demonstrated that the probability of replication across multiple independent populations is very small when power of detection is not close to 1, even in the simplest case where the true variant is purely additive.

Compounding the difficulty of replication, \citet{Greene2009} suggested that this problem is exacerbated further if the marginal effect is involved in a pairwise interaction. The marginal effects of two locus interactions for functional genotype-phenotype map tend to be highly dependent upon allele frequencies, and allele frequencies are liable to fluctuate across populations. Thus, it can be argued that searching for marginal effects alone will fail to replicate from one population to the next if they are involved in an interaction. \citet{Greene2009} went on to suggest that two-dimensional searches would alleviate this problem, and while this is true to some extent it also introduces other potential complexities. First, power to detect epistatic interactions is generally lower than marginal effects, so the problem of statistical replication, even with knowledge of the true interaction terms, will be inflated. Second, extrapolating their argument, the two-dimensional effects may be marginal to higher order interactions, in which case they are liable to the same fluctuations in allele frequencies across populations as are independent effects. And third, the interaction may be with environmental factors rather than other genetic loci, and this is a very complicated problem to control when comparing different populations.

In addition to these problems there are of course others. For example the initial association may be in LD with the true causal variant in one population but not the other, or the causal variant may simply not be segregating in other populations. Less stringent definitions for what constitutes a replication could be made with regards to this. For example, replication could entail searching for the same effect across all SNPs covering the same gene, all SNPs covering genes involved in the same pathway, or including the same gene ontology (GO) terms \citep{Cantor2010}. If robust statistical corrections are made for the increased multiple testing then this could ostensibly improve the power of detecting genomic regions with relevant functions, however such an approach does depart from the original problem of understanding genetic variation because without validating a candidate variant then the estimate for its mode of action remains unconfirmed. As discussed in chapter 5, the problems associated with incomplete LD will tend to inflate the importance of additive effects.

True variants are demonstrably difficult to replicate, but there is still some concern as to whether replication is in itself a strong foundation for belief of biological function as it can be shown that the probability of replication of a false positive can be fairly high under certain situations \citep{Liu2008}. Consequently, replication in independent populations, while commonly considered the gold standard of statistical association, probably still does not go far enough to address the question of true biological effect. \citet{Chanock2007} suggests that the more laborious procedure of candidate interval sequencing and genotyping of all regional common and uncommon variants in multiple populations should also be performed, followed by examination of functional consequences, and gene and environment interaction effects. Of course functional effects \emph{in situ} may be different from those examined in laboratory conditions, and there is no easy way to comprehensively measure the environmental conditions that may be involved. Further, under these guidelines the dissection of even a single variant is likely to be time consuming and costly, and ascertaining such levels of confidence for all variants associated with highly polygenic traits is a daunting task.


\subsection{Genetic prediction}

Alternative approaches exist that simply bypass the problem of stringent thresholds. The discovery of causal variants is important for two major reasons. First, it pinpoints areas of the genome that are functionally related to a phenotype of interest. Second, one can use this information to predict phenotypic outcomes. While epistasis may be important in the first instance in order to detect the causal variant and to understand the context in which the function occurs, its significance in the second instance may manifest in terms of the accuracy of the prediction.

In animal and crop breeding, where a principle objective is to select individuals with the most beneficial characteristics for genetic contributions to future generations, genomic selection has dominated quantitative genetic theory over the past decade. Whereas GWAS treats each SNP independently as fixed effects, genomic selection attempts to fit all genetic factors simultaneously as random effects in order to estimate the genomic breeding value (GEBV) of each individual \citep{Meuwissen2001}. This can be achieved through many methods \citep{Gianola2009}. Though no single approach is currently deemed superior above all others, particularly popular are those that parameterise all effects as additive and then use Bayesian sampling techniques in conjunction with highly sparse prior distributions for the variances of all loci. Here, the identification of individual factors or regions is largely inconsequential, and thus problems involving stringent thresholds are not relevant. Ultimately, genomic selection has been fairly successful, improving upon methods that involved progeny testing and delivering improved commercial productivity. For example, the accuracy of GEBVs in dairy cattle for milk yield are around 67\%, close to twice the accuracy of traditional pedigree-derived breeding values \citep{Harris2008}, and genotyping of elite animals is now becoming routine \citep{Hayes2009}.

Perhaps these types of studies can provide clues as to the statistical importance of epistasis. For example, if functional epistasis is prevalent amongst causal variants for milk yield, but prediction accuracy remains high with an additive parameterisation then this may suggest that interaction terms make a relatively small contribution to the variance, however without knowing the performance of such models in the hypothetical case that there are only epistatic terms contributing to the variance it is difficult to form any strong conclusions. Perhaps one can also use the results from genomic selection to begin to ascertain the polygenicity of a trait. A recent study attempted to apply genomic selection techniques for prediction in human height \citep{Makowsky2011}. For this trait, the REML estimate of the variance explained by the additive genomic relationship matrix was approximately 45\% \citep{Yang2010}, but both BLUP and Bayesian LASSO estimates, when 10-fold cross validated, resulted in an average prediction accuracy of less that 15\%. It has been shown for an additive polygenic model that the accuracy of prediction $r_{g\hat{g}}^{2}$ is constrained by the ratio of number of individuals $n_{p}$ to the number of causal loci $n_{G}$ and the observed heritability $h^{2}$ \citep{Daetwyler2008}:
\begin{equation}
r_{g\hat{g}} = \sqrt{\frac{\frac{n_{p}}{n_{G}}h^{2}}{\frac{n_{p}}{n_{G}}h^{2}  + 1}}.
\label{eq:daetwyler}
\end{equation}
One interpretation of the poor performance in prediction could simply be that the trait is highly polygenic. The study comprised 1493 individuals in the training set, so rearranging equation \ref{eq:daetwyler} to $n_{G} = r_{g\bar{g}}^{-2}(n_{p}h^{2} - n_{p}h^{2}r_{g\bar{g}}^{2})$ gives an estimated $n_{G} = 3807$. Given that the training and testing sets were from the same population and the prediction was based on cross validation one might expect that any systematic errors arising from interaction between genetic or environmental factors might be minimised. Thus, applying the estimated prediction model to other populations, where environmental conditions and allele frequencies will differ, may provide insight into the prevalence of interactions: if accuracy remains at 15\% then the most effective way forward for this particular trait would be to increase sample size, but if the accuracy drops then the genetic model may require revision.

Assuming that the inclusion of epistatic components will improve this type of study it still remains unclear how they might be incorporated into the prediction model. For example, one approach might be to simply obtain estimates based on the interacting pairs uncovered from two dimensional GWAS, however the variance explained of all significant factors combined would need to be high in order to achieve any reasonable level of accuracy \citep{Wray2007, Evans2009}. Alternatively a relaxed threshold could be used to allow a larger number of features with small effects to be utilised for prediction. However this has previously offered only small advantages in prediction for highly heritable traits such as schizophrenia \citep{Purcell2009}. Several methods have been proposed that could include non-additive variance components directly \citep{Gianola2006, Gianola2009, deLosCampos2009} and if the numerical difficulties associated with their estimation from genomic relationship matrices may be overcome with larger sample sizes then this may be an interesting direction to explore the global statistical impact of non-additive terms.


\section{Final remarks}

That high level morphological characters are manifested by the combination of discrete Mendelian processes implies an underlying granularity to the observed noisiness of biological systems, and GWA style approaches attempt to dissect this directly. But perhaps this is an overly simplistic representation, as stochasticity is likely to exist at every level higher than the genetic factors themselves, and it appears that overcoming this problem cannot be achieved without compromise. Genomic selection techniques sacrifice detail for prediction accuracy, whereas migrating GWAS to lower level phenotypes will sacrifice prediction accuracy for detail, and reconciling both approaches is evidently not straightforward.

There are a wide range of opinions as to the reasons behind the problem of the missing heritability \citep{Eichler2010}. While all valid, many of them are rather esoteric, invoking such concepts as the rare variant hypothesis, copy number variations, genetic imprinting, and non-additive genetic variance. But perhaps the most immediate problem is that with highly polygenic traits there is no strong basis to reject (or accept) the common disease-common variant hypothesis or the additive genetic paradigm. It might be argued that for the purposes of understanding the architecture of genetic variation genomic selection approaches are fairly subjective in terms of statistical interpretation. On the other hand, integrating a phenomic approach into the GWA framework has clear advantages for improving statistical power, and if the associated problems of causality can be addressed (\emph{e.g.} intsrumental variables \citet{McKeigue2010}), then one might speculate that both detailed functional information as well as good prediction accuracy could be obtained from genomic information.

Additive parameterisations have dominated the data mining techniques for causal genetic polymorphisms in recent years but the question of the type of genetic variance that underlies complex traits remains an important one to resolve. It is unlikely that a single solution exists, and the architecture of one trait may be entirely different to the architecture of another. Ultimately, epistasis may or may not have an important role in complex traits, but a theoretical precedence for it has long been established. Today, the necessary data is abundant and the data mining tools have been developed, so the true importance of epistasis can now begin to be qualified empirically.


