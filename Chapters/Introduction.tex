% Introduction

\chapter{General introduction}
\label{Introduction}
\lhead{General introduction}

A fundamental objective of genetics is to forge a connection between phenotype and genotype. But with the complexity that ubiquitously governs biological systems this can be a daunting task, and it could be argued that the biggest failure in biological sciences over the past decade has been in the mapping of phenotypic variation to variation in the genome. This has prompted much debate regarding the fundamental nature of genetic variation, a question of crucial importance to the advancement of many fields such as medicine, animal and crop breeding, and evolutionary theory. This thesis seeks to develop novel methods for the detection of genetic variation that impact phenotypes with a particular emphasis on mapping interacting genetic factors. In this chapter the current understanding of the architecture of genetic variation is discussed, following a brief introduction of the historical paths that lead to today's practices in quantitative genetics.


\section{Historical foundations}

Prior to the rediscovery of Mendel's theory of genetic inheritance in 1900, there existed a division in the scientific community regarding the nature of evolution and the mechanisms that governed natural variation. Darwin's \emph{The Origin of Species} (1859) is celebrated today for first describing the mode of evolution in terms of natural selection, and creating the framework for the ``gradualist" school of thought, but it was not immediately accepted, and the emergence of several other major concepts were required before these ideas would eventually become solvent. At the time the alternative view was that evolutionary changes occurred not continuously but in discrete steps, and this ``saltatory" theory was often the more popular school of thought. Perhaps the biggest scientific criticism of gradualism was that it was incompatible with the then prevailing understanding of the transmission of hereditary material, blending inheritance. Under the theory of blending the variance of a character would disappear by a factor of a half each generation thus eventually insufficient variation would remain for gradual selection to act upon. In contrast, proponents of saltation were largely supported by cases from botanists and horticulturalists who often observed the sudden origin of deviant types in crops in agriculture. 

Post rediscovery of Mendelism, for a time the acrimony actually only deepened. The particulate nature of genetic material was intuitively appealing to the saltatory perspective, and for a time it was a common position to believe that Mendelism had destroyed Darwinism. It was not until the mathematical treatment of Mendelian inheritance in populations was developed through the Hardy-Weinberg principle in 1908 \citep{Hardy1908, Weinberg1908} that the theories of Mendelism and Darwinism were reconciled. It demonstrated that excluding any external forces, namely selection, the transmission of genetic variance through generations was shown to be stable - that is to say that the expected frequencies of neutral alleles for the next generation are equal to the observed frequencies in the current generation. As such, the criticisms applied to Darwinism in the context of blending inheritance were no longer pertinent under Mendelian inheritance, because genetic variation will be maintained. Buttressing the gradualism paradigm was the proposal of the infinitesimal paradigm \citep{Fisher1918}, which modelled adaptation on the assumption that mutations with large effects would likely be deleterious and thus rapidly purged from the population, and that adaptation occurred through the action of innumerable minute effects.

Still today the maintenance of genetic variation is not fully understood, and the evolutionary perspective of saltation, now more commonly referred to as punctuated equilibrium, has not been resolved. But the generic mechanism that underlies the relationship between genotype and phenotype had been established fairly convincingly and the foundations for population and quantitative genetics had been laid. The next section will follow the progress that has led from identifying the first molecular markers for genetic variation, to the situation today where whole genomes are being sequenced.


\section{Genetic variation}

The human genome comprises approximately three billion base pairs but surprisingly, with current predictions for the total number of genes at 20-25 thousand, only 1.5\% of this encodes for proteins \citep{InternationalHumanGenomeSequencingConsortium2004}. Estimates for the total proportion of the genome that is functional vary, but generally remain extremely low with comparative analysis suggesting a range of only 2.5 - 5\% \citep{Waterston2002, Chiaromonte2003, Lunter2006}. These numbers are remarkably small, yet paradoxically they do in many ways suggest that the genome is actually more complex than was previously imagined. For example, if there were a much higher number of protein coding genes, and a large proportion of the genome was functional then one could propose a granularity in the operations of each component \citep{Piatigorsky1989}. But in reality proteins seldom act independently, and many proteins are members of several independent complexes, playing different roles in separate aspects of the cell machinery \citep{Jeffery2003}, under the governance of multiple levels of spacial and temporal regulation. And so although the exact number of proteins is small, combinatorially the number of potential complexes is astronomically large. It is the ubiquity of protein interaction that belies the complexity of the genome.

The impact of mutation on such a system is widely studied, but the architecture of genetic variation in natural populations is largely unknown. With advances in genotyping technology it is now becoming possible to tackle such questions. Each individual gains approximately 60 \emph{de novo} mutations through meiosis in both parents \citep{Conrad2011}, and on average 1 in every 1200 base pairs will differ between any two individuals \citep{Sachidanandam2001}. Indeed, given the mutation rate of $\sim 10^{-9}$ it is expected that each generation a mutation at every base pair occurs somewhere in the population, and over evolutionary time the forces of selection and neutral drift cause some of these mutations to become common polymorphisms in the population. The ability to measure genetic variation is of great utility to many aspects of biology but perhaps the most common goal is to understand phenotypic variation in terms of genetic mutation. Because of the potential medical and agricultural benefits that can be gained from this endeavour there has been over the last few decades a rapid development in the technology.

An important conceptual addition to simple Mendelian inheritance is linkage. Formalised by \citet{Morgan1911} following the initial discoveries by Bateson (1905) and Punnett (1905), linkage is the tendency for certain characteristics to be inherited together more frequently than by chance due to limited recombination between them. This can be defined probabilistically as a function of the number of recombination events that are expected to occur between two positions on a chromosome based on the distance separating them. An early example was presented in 1923 where it was shown that size differences in \emph{Phaseolus vulgaris} could be predicted by their seed coat pigmentation \citep{Sax1923}. While discoveries of linkage between molecular variation and phenotypic variation continued for some time (\emph{e.g.} colour-blindness and haemophilia \citep{Bell1937}, blood types and disease \citep{Lawler1959}), perhaps the first attempts to link genetic variation with phenotypes were through karyotype analysis, the direct observation of chromosomal abnormalities through the use of staining techniques. Though low resolution, it is possible to identify large structural abnormalities given a reference set of natural chromosomes. The first successful case of mapping a human disease phenotype to alterations in genetic material was presented by \citet{Nowell1960}. Known as the ``Philadelpha chromosome", it was found that a reciprocal translocation between chromosome 9 and 22 was associated with chronic myelogenous leukemia. Subsequently there have been many structural chromosomal abnormalities or aneuploidies discovered to be associated with various diseases, and through identification and study of the interrupted genes advancements have been made in the understanding of the aetiology of these conditions.

The journey toward a finer resolution of genetic variation continued with the discovery and isolation of restriction endonucleases, enzymes that cleave DNA at specific short sequences \citep{Danna1971}. Because inevitably there will be variation in the positions of the restriction sites in different individuals, the resulting digested DNA fragments will also vary in length. Through gel electrophoresis these differences can be characterised, and linkage between a restriction fragment length polymorphism (RFLP) and a phenotype was first demonstrated by \citet{Grodzicker1974}. Subsequently their usage increased and by 1980 a putative genetic map of the human genome had been produced \citep{Botstein1980}. The eventual first genome-wide search for quantitative trait loci (QTL) in experimental organisms \citep{Paterson1988} heralded a significant conceptual breakthrough in detecting regions of the genome that may be involved in the genetic control of variation without any prior knowledge of where to begin the search.

Many other methods also exist and have been used effectively, but the next major breakthrough in assaying variation in populations was enabled by combining discoveries from several areas: the ability to amplify DNA polymerase chain reaction (PCR), the discovery of evolutionarily conserved sets of PCR primers, the routine sequencing of short regions of DNA, and the discovery of microsatellite variation. Microsatellites are short (1-6 base pairs), repeating sequences of DNA that encapsulate many desirable features of a genetic marker. Their utility comes from the combination of many features: they are co-dominant, usually selectively neutral and widely distributed throughout the genome, highly informative because each locus can have multiple alleles, and relatively cost effective \citep{Jarne1996, Sunnucks2000}. Genetic maps composed of microsatellites were developed for many species, including humans \citep{Weissenbach1993} and several livestock species \citep{Womack1993, Rohrer1994, Groenen1998}. Through least squares adaptations \citep{Haley1992, Haley1994} of the original maximum likelihood formulation for linkage analysis \citep{Lander1989} it became computationally tractable to construct more sophisticated QTL models in a multiple regression framework.

Today the principle measurement of variation is through biallelic markers known as single nucleotide polymorphisms (SNPs). Although individually microsatellites are more informative by virtue of being multiallelic, SNPs are vastly more abundant, and so along with providing more thorough coverage of the genome, an important requirement in non-structured populations, when used as a set of features they are significantly more informative also. The Human HapMap project was created with the goal of creating a catalogue of human genetic variation from four populations with African, Asian, and European ancestry, with a view to understanding the distribution of common polymorphisms between and within populations, and how they are involved in phenotypic variation \citep{TheInternationalHapmapConsortium2005}. The 1000 genomes project has a similar goal \citep{Durbin2010}, and together with the advent of high-throughput genotyping and sequencing techniques a very detailed index of human genetic variation is being compiled.

Because of the abundance of SNPs in the genome and the rapidly growing availability of this type of data, they form the focus of the studies in this thesis. But it should be noted that other types of sequence variation are coming under scrutiny also. For example, copy number variants (CNVs) - alterations in the number of copies of large sections of DNA ranging from 1 kilobase (kb, 1000 nucleotide bases) to many megabases (mb, 1 million nucleotide bases) - are estimated to account for 12\% of the genome \citep{Armengol2009, McCarroll2010}. And to complicate things further, genetic variation can exist beyond alterations in the actual DNA sequence, for example methylation of cytosines has an inhibitory effect on transcription and these molecular modifications can be inherited through the germline (genetic imprinting) \citep{Petronis2010, Heard2010, Danchin2011}.

We are beginning to construct an image of a highly variable genetic code, and although cheap sequencing of entire genomes is fast becoming a reality it should be noted that while this has the potential to accurately survey all variation in one dimension, DNA exists in a complex three dimensional structure \citep{Duan2010} that suffers from somatic mutations and modifications over time \citep{Pleasance2010}. Nevertheless, with the availability of whole sequence data for large samples of the population imminent, a realisation that has been a century in the waiting, this is an exhilarating time in genetics. The statistical techniques and experimental designs have changed with the evolution of the data, and will be required to change again, and the next section will introduce some of these concepts.


\section{Phenotypic variation}

\subsection{Simple Mendelian traits}

Traditionally, structured populations have been used in the mapping of causal genetic variants to phenotypes. In the case of experimental organisms it is possible to create genetically uniform strains in order to fix for particular characteristics, and then through cross breeding one can begin to search for putative regions of interest through linkage analysis. The same principle applies in non-experimental organisms where the expected genetic correlations of relatives can be exploited as a contrast to the weaker between-family relationships. Briefly, linkage analysis predicates upon the differences in covariances between the identity-by-decent (IBD) statuses of candidate loci, and depends on these types of population structures to maintain linkage between causal variants and observed markers, the co-segregation being liable to breakdown after relatively few meioses. Thousands of such studies have been performed over the last few decades and they are particularly effective at identifying the underlying polymorphisms involved in ``simple Mendelian traits" - phenotypes that, through pedigree studies, can be shown to depend only on a single genetic factor.

There have been many notable successes in human studies using linkage analysis, the first of which occurring even before the commencement of the Human Genome Project. Cystic Fibrosis was known to be a recessive autosomal disease and through linkage analysis a region of chromosome 7 was identified as the causative locus \citep{Kerem1989}. Through chromosome walking \citep{Rommens1989} and positional cloning \citep{Riordan1989} the protein that is mutated in patients, dubbed CFTR (cystic fibrosis transmembrane regulator), was discovered. While this was a huge milestone in genetics, it should be noted that to call the genetic variation underlying cystic fibrosis ``simple" is somewhat misleading from both a genetic and a phenotypic perspective. For instance, by 2002 more than 1000 causative mutations within this gene had been identified in different patients \citep{Salvatore2002}, and the variation in the severity of the disease amongst patients can range from male infertility being the only symptom to multiple organ disruption, and from survival for many decades to death within the first 10 years. The extent of this variation cannot simply be explained by the CFTR mutant type, and while affected siblings tend to exhibit similar pancreatic symptoms \citep{Corey1989} the severity of pulmonary disease is highly variable \citep{Kerem1990}. Multiple genetic factors in addition to environmental factors are likely to be involved. For example susceptibility to infection, itself under strong genetic control, has been shown to be have an important role in pulmonary status of cystic fibrosis patients, and so while the broad phenotype is simple the prediction of severity and prognosis is potentially highly complex.

\subsection{Complex traits}

Indeed, such complexities within simple Mendelian diseases are the rule rather than the exception \citep{Summers1996}. Along with severity and prognosis, other variable factors can include penetrance, pleiotropy, and environmental specificity. Most commonly these attributes are outside the control of the major disease locus, and may comprise multiple genetic and environmental factors. Indeed it may be thought curious to discover an important phenotype to be under the control of a single locus because they should be eradicated through purifying selection. In the cases where they are found, one example being CFTR mutants in cystic fibrosis, the polymorphism is likely to be maintained through some form of balancing selection, such as heterozygote advantage \citep{Jorde1988}. Most major diseases in developed nations are complex in nature, as is the case for many phenotypes important in agriculture, and therefore with a large number of factors contributing to the variance, each effect will be small. As demonstrated by \citet{Risch1996}, linkage studies are poorly suited to detecting small effects, and a more powerful approach is to use association analysis. Rather than depending on linkage, the dynamic model of co-segregation between markers and QTLs, association studies exploit linkage disequilibrium (LD) between observed markers and causal variants on ancestral haplotypes. LD is the assortment of alleles in combinations that are more or less frequent than would be expected based on population frequencies of the individual alleles. In the case that a marker is in high LD with a causal variant then the allelic or genotypic effect will be similar, and so the statistical framework for such analyses is fairly straightforward. Ostensibly it is based upon testing for differences in the effects on the trait for different genotype classes, with the implicit assumption being that the marker under test is either the causal locus itself, or else is very close by such that the effect estimate is a good approximation of the true effect.

While conferring greater statistical power to detect small effects in structured populations, in conjunction with the rapidly growing density of population SNP data, association style statistical approaches can be applied to studies where the population based samples are largely unstructured. In unstructured populations there are likely to have been a high number of meioses since the time of the last common ancestor between any two individuals, so long range associations are disrupted and the LD will decay rapidly as markers become more distant from one another. To be sure of capturing a sufficiently large proportion of the population's genetic variation it is necessary to have a very dense array of markers. Typically 300,000 markers and above are used in human genome wide association studies (GWAS) (although fewer are sufficient for other species with more recent population bottlenecks), giving an average physical distance of 10kb or less between markers. But of importance is not only SNP density, but allelic spectra also. The distribution of allele frequencies in natural populations is typically `U' distributed, there being many more rare variants than there are common ones. However, the principle under which GWAS operates follows the common disease-common variant (CDCV) hypothesis. CDCV was proposed \citep{Lander1996} following the observation that several common polymorphisms ($> 5\%$ frequency in the population) were already known to confer increased risk for complex phenotypes such as Alzheimer's disease (Apolipoprotein E, \citet{Strittmatter1993}), heart disease (ACE, \citet{Kreutz1995}), and HIV susceptibility (CKR-5, \citet{Liu1996}). In addition there were practical reasons for concentrating on common variation in the first instance. It is much easier to catalogue common polymorphisms in the population through resequencing as it requires a much smaller sample, and from a statistical perspective LD between markers is maximised when the frequencies of the two markers are identical \citep{Schork2000}. So in effect, when SNP genotyping arrays began emerging with a near-uniform distribution of minor allele frequencies, typically ranging from 0.05 - 0.5, an implicit prior assumption was being imposed on GWAS that the causal variants for the phenotype under study were also common.


\section{Partitioning the variance}

\subsection{Heritability estimation}

Over the last decade hundreds of large scale GWASs have been performed \citep{Hindorff2010}, and thousands of variants have been discovered for a plethora of complex human traits. However the proportion of the variation that is predicted to exist in these traits is typically dramatically higher than the proportion explained by the mapped variants \citep{Maher2008}. Although the focus of this thesis is on the detection of genetic interactions, and their potential impact on the problem of the so-called `missing heritability' is discussed in detail in chapter \ref{Results1}, other factors may also play an important role, and these are discussed below.

The underlying premise for any GWAS is that contributing to the variance of the phenotype are both genetic and non-genetic factors, and as one of the general aims is to map the variance that comprise the genetic component it is therefore necessary to estimate the proportion of the phenotypic variance that is genetic. The simplest approach to calculate the total genetic variance as a proportion of the total phenotypic variation, or the broad-sense heritability ($H^2$), is often performed in plant studies \citep{Soleri2002, Nordborg2008, Xu2009}. Here several different varieties of a species are cloned, the within variety variance of the trait becomes an estimate of the environmental contribution and the between variety variance is considered the proportion of the variance that is genetic. However, there are two major problems with this type of study. Firstly, clonal experiments cannot be performed easily for most animal species; and secondly, the broad sense heritability when estimated in this manner provides no clues as to the mode of action of the genetic factors. For any SNP with an effect on a trait the mode of action can be parameterised into two components: the additive allelic effect $a$, classically defined as half the difference between the opposing homozygotes; and the dominance interaction $d$, or the within locus interaction between alleles, calculated as the deviation from the mid-homozygote value. The additive variance of the locus can be estimated as
\begin{equation}
\sigma^{2}_{A} = 2pq[a + d(q - p)]^{2}
\end{equation}
and the dominance variance
\begin{equation}
\sigma^{2}_{D} = (2pqd)^{2}
\end{equation}
where $p$ and $q = 1 - p$ are the frequencies of the two alleles. Consequently, the estimated variance of a locus is heavily dependent upon the frequencies in the population, as is the estimated ratio of additive to dominance variance.

Further complications arise when considering the joint effect of two or more loci, in the context that the sum of the marginal variances of each locus may be less than the estimate of the total genetic effect of all loci when considered jointly. This synergistic relationship is known as epistasis, or gene interaction, and it typically arises when the phenotype manifested by a locus depends on the genotypes at other loci \citep{Carlborg2004}. Extending the parameterisation to explicitly include epistatic components, rather than treating all non-marginal effects as a single residual genetic component, can be performed directly. \citet{Kempthorne1954} introduced a partitioning method for a two locus bi-allelic system comprising 4 interaction terms, $additive \times additive$, $additive \times dominance$, $dominance \times additive$ and $dominance \times dominance$, in addition to the 4 marginal effects at two loci described above, whereby each interaction term was the deviation from the underlying marginal terms. For example, the estimated $additive \times dominance$ effect would be the deviation from the joint effects of the allelic effect at the first locus and the genotypic effect at the second. An alternative early parameterisation modelled the 8 genetic components as orthogonal contrasts of the genotypic values \citep{Cockerham1954}. Both assumed equal allele frequencies (\emph{e.g.} from an F2 population), Hardy-Weinberg equilibrium, and linkage equilibrium between interacting loci. By creating an easily interpretable statistical framework for the estimation of the synergy that might exist between loci it became possible to begin characterising the types of genetic effects that exist in nature.

As experiment design has developed so have the statistical frameworks for epistasis. \citet{Kimura1965} created the first parameterisation that orthogonally estimated the additive variance in a two locus model when the markers were in LD, and \citet{Mao2006} extended the Kempthorne parameterisation to account for both LD and Hardy-Weinberg disequilibrium. Another important revision was the NOIA parameterisation (natural and orthogonal interactions) \citep{Alvarez-Castro2007, Alvarez-Castro2008}, which created a general extension of the Cockerham model to be orthogonal at all frequencies and under Hardy-Weinberg disequilibrium for any number of interacting loci. These important developments have made the estimation of genetic effects in a GWAS context statistically soluble, albeit extremely computationally difficult. However the problem of partitioning the total genetic variance in a trait remains problematic.

There are several methods in general use that estimate the genetic component of a phenotype, but crucially they are predicated upon the estimation of additive variance only, or the narrow-sense heritability ($h^2$). This is true for several reasons. From a theoretical point of view it is common for quantitative trait values of offspring to correlate with the mid-parent mean \citep{Fisher1918}. From a practical point of view, estimating the additive variance is relatively easy in non-clonal populations, and in particular when pedigree information is available. The covariance between two individuals is
\begin{eqnarray}
cov(x,y) &=& \Theta_{x,y}\sigma^{2}_{A} + \Delta_{x,y}\sigma^{2}_{D} + \nonumber \\
&& \Theta_{x,y}^{2}\sigma^{2}_{AA} +  \Theta_{x,y}\Delta_{x,y}\sigma^{2}_{AD} + \Delta_{x,y}^{2}\sigma^{2}_{DD} + \nonumber \\
&& \sigma^{2}_{C}
\end{eqnarray}
where $\Theta$ is the coefficient of kinship - the probability that an allele chosen at random will be IBD between $x$ and $y$, and $\Delta$ is the coefficient of fraternity - the probability of both alleles at a random locus being IBD \citep{Fisher1918, Jacquard1974}. In the equation above the first line represents the marginal components, the second line the interaction terms and the third line the common environment between $x$ and $y$. In human genetics the classic method of estimating $\sigma^{2}_{A}$ is through twin studies. Briefly, monozygotic (MZ) twins are expected to be genetically identical, so they will share the same additive and dominance variance, and a very similar common environment. Dizygotic (DZ) twins on the other hand will on average share only 50\% of alleles and 25\% of genotypes. Assuming the classical additive model and ignoring other variance components it is expected that the correlation of phenotypes between MZ twins will be twice that of DZ twins. Many twin studies have been performed that indeed confirm this model, but it may be premature to discount other forms of variances. For example (ignoring epistatic components for simplicity) through the following equation
\begin{equation}
\frac{cov_{DZ}(x,y)}{cov_{MZ}(x,y)} = \frac{\frac{1}{2} \sigma^{2}_{A}}{cov_{MZ}(x,y)} + \frac{\frac{1}{4} \sigma^{2}_{D}}{cov_{MZ}(x,y)} + \frac{\sigma^{2}_{C}}{cov_{MZ}(x,y)}
\end{equation}
one would expect the ratio to equal $0.5$ if the genetic variance was entirely additive. However, if the ratio is greater than $0.5$, then there would be the expectation of a shared environment component, but if less than $0.5$ then one might expect dominance effects (and/or other non-additive genetic components) contributing to the phenotypic variance. If both non-additive genetic components and $\sigma^{2}_{C}$ contribute to the trait then the ratio will tend toward $0.5$, and one might incorrectly conclude that the genetic affect is purely additive. So heritability estimates performed through twin studies alone can be criticised because estimating both $\sigma^{2}_{D}$ and $\sigma^{2}_{C}$ simultaneously is negatively confounded \citep{Evans2002}. Another criticism is that twins may differ from singletons, so the estimation of genetic variance may not generalise to the population \citep{Petterson1993, Phillips1993, Record1970}, but interestingly it has also been suggested that such differences are eventually overcome after early development \citep{Posthuma2000}, suggesting a mechanism of `canalisation' during development \citep{Waddington1942}. In any event, although common practice, it is perhaps insufficient to base the estimate of additive variance on such studies alone, and it has been suggested that if large pedigrees are available then using other types of relationships that will have differing expected coefficients of variation for different components \citep{Hill1982} may be one way to tease apart the confounded factors \citep{Haley1981}.

Yet even with more informative pedigree studies there are several factors that have the propensity of resulting in biased overestimates of the additive genetic component. One major effect that is extremely difficult to measure is $gene \times environment$ interaction $G \times E$. Statistically such factors result in heteroscedasticity because for example the variance in one environment where the effect of some genetic factor is released will be larger than in other environments where the effect may be masked. A related problem, $GE$ correlation, occurs when groups of individuals with a certain genotype have the tendency to segregate in similar environments. Naturally this causes an increased resemblance amongst relatives and will inflate the estimate of genetic variation. Finally, statistical models for estimating heritability typically depend on the assumption of random mating. However for many traits, particularly a problem in human studies more than controlled animal breeding, there is likely to be (positive) assortative mating - the tendency for individuals to mate with those with a similar phenotype. This causes an increase in homozygosity of the underlying genetic factors for the trait, should they exist, and also causes directional pseudo-LD \citep{Kimura1965}. Both factors lead to an increase in the additive variance as they increase the phenotypic correlation between parents. Ostensibly, heritability estimates are a ratio of variances, and as such the magnitude of effects are not considered. As genetic factors may have impacts in one environment while being neutralised in others, it must be remembered that such estimates are often highly subjective.


\subsection{The missing heritability}

Ultimately, there are a myriad of complications in estimating the genetic variance of a trait, and in decomposing the genetic variance. Many of these complications result in an overestimate of the additive variance, and GWASs have been almost exclusively performed under the assumption of a polygenic additive model. An alternative method of estimating $h^2$ is to use dense SNP information in unrelated populations. Estimation of the kinship matrix based on the sharing of alleles IBD at hundreds of thousands of markers can be used to construct genomic additive relationships $\Theta_{G}$, even in the absence of pedigree information, and REML estimates of $\sigma^{2}_{A}$ can be made in this fashion. Unfortunately, although some of the concerns of confounding are potentially less prevalent amongst unrelated samples, because the coefficient of fraternity is typically much smaller than the coefficient of kinship the dominance variance is extremely difficult to estimate stably, as is the case for the higher epistatic components (\emph{e.g.} $A \times A$ is the square of $\Theta_{G}$). Nevertheless, when estimates of the heritability are performed using the genomic relationship in unstructured populations they are typically much smaller than those from structured populations using pedigree based relationship matrices. For example, height has a pedigree based estimate of $h^2 \approx 0.8$, but a genomic based estimate of only $h^2 \approx 0.5$ as estimated from both unrelated populations \citep{Yang2010} and full-sib cohorts \citep{Visscher2007}. Clearly one source of this discrepancy could arise from inflation of the pedigree based estimate. However while SNP information is extremely dense it is still an incomplete representation of the total genomic variation, and combined with the ascertainment bias in the distribution of allele frequencies there may also be a deficit in the genomic based estimate. Extrapolating, if the genomic relationship matrix fails to capture the genetic variance simply because neutral SNPs in the observed array have a different distribution to causal variants then this is one potential reason for the poor performance of GWAS. Indeed several studies have suggested that rare variants (frequency $< 1\%$) may be more important for complex traits than common variants \citep{Eyre-Walker2010}.

Currently circulating are many alternative explanations too. Perhaps the simplest revisits Fisher's infinitesimal model \citep{Fisher1918} and suggests that the polygenicity is too high, so correspondingly the effect of each factor is very small, thus requiring sample sizes much larger than the ones in general current use for detection. More esoteric considerations can also be made. \citet{Li2008} notes that although individually each rare variant can explain very little of the variance of a trait, collectively rare variants are extremely common, and that given sufficient polygenicity such polymorphisms could account for the large genetic variances. With the possibility of widespread pleiotropy such an assumption is tenable and indeed, the rare variant hypothesis has been shown to be particularly cogent for traits under selection, relative to CDCV \citep{Eyre-Walker2010}. A similar idea has been proposed by \citet{Eichler2010} but in the context of non-SNP polymorphisms. While large polymorphisms, (\emph{e.g.} $\geq$500kb deletions or duplications) are individually rare, they are collectively common, existing in $\sim 8\%$ of European populations \citep{Itsara2009}. Invoking ideas of epistatic canalisation or capacitance \citep{Waddington1942, Bergman2003} it has been suggested that the variance of most alleles are only released in the presence of certain large polymorphisms, so attempts at mapping without this consideration will be underpowered and incorrectly parameterised. Other arguments that suggest a role for interactions in the additive variance have also been made. \citet{Haig2011} postulated that if certain epistatic variants are in LD with one another then some configurations can manifest purely additive variance, while the SNP effects will be undetectable through an additively parameterised GWAS. Perhaps even more complicated is the potential role of epigenetics. Interactions between DNA methylation sites and genetic polymorphisms could have similar capacitance effects, and they could also act in a purely additive manner that would contribute to the heritability independent of genetic variation \citep{Petronis2010}. Currently the molecular basis for epigenetic inheritance is unknown but it is likely to be modulated through relatively poorly understood genetic material such as small RNA molecules. To summarise, the once consensus view of genetic variance being mostly comprised of an additive component consisting of common, independent additive polymorphisms was invoked using the principle of Occam's razor. The infinitesimal model accommodates this type of architecture, but many of the prevailing thoughts on the matter invoke much more complex mechanisms. Perhaps the observation of the existence of additive variance is symptomatic of a more complex underlying genetic architecture, and this leads to an often neglected question - what comprises the remaining phenotypic variation? It must be noted that there is a dearth of information regarding potential non-additive genetic variation for traits of importance, and with current variance components techniques unable to accurately gauge the extent of their importance there is a need to develop tools and methodology in this area.


\section{Epistasis}

Soon after the emergence of Mendelism it was frequently observed that genetic factors did not act independently. Literally translating to ``standing upon", epistasis was first defined by Bateson in 1908 to describe the observation that in the segregation ratios of comb types in chickens one particular type only manifested in the rare double recessive homozygote class. Subsequently many other exotic segregation ratios were discovered, again only explainable using two Mendelian loci (table \ref{tab:segrat}, after \citet{Snyder1931} and \citet{Phillips1998}).

\begin{table}
\begin{center}
\begin{threeparttable}
\caption{\label{tab:segrat}Early discoveries of two locus segregation ratios}
\begin{tabular}{| l | c | c | c | c |} \hline
Interaction type\tnote{a} & A-B- & A-bb & aaB- & aabb \\ \hline \hline
Classical ratio & 9 & 3 & 3 & 1 \\ \hline
Dominant epistasis & \multicolumn{2}{ c |}{12} & 3 & 1 \\ \hline
Recessive epistasis & 9 & 3 & \multicolumn{2}{ c |}{4} \\ \hline
Duplicate gene with cumulative effect & 9 & \multicolumn{2}{ c |}{6} & 1 \\ \hline
Duplicate dominant genes & \multicolumn{3}{ c |}{15} & 1 \\ \hline
Duplicate recessive genes & 9 & \multicolumn{3}{ c |}{7} \\ \hline
Dominant and recessive interaction\tnote{b} & \multicolumn{2}{ c |}{13} & 3 & - \\ \hline
\end{tabular}
\begin{tablenotes}{\footnotesize
\item[a] Additive case (no interaction)
\item[b] Class aabb segregates with A-B- / A-bb class}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}


This framework for identifying epistasis, also known as functional or physiological epistasis, is more broadly defined today as being the phenomenon that a particular genotype's effect is dependent on its genetic background. But as discussed above an alternative interpretation is also in common use where epistasis is defined as the statistical deviation from the summed or multiplied (depending on the phenotypic scale) marginal effects of each locus \citep{Fisher1918, Kempthorne1954}. For the purposes of understanding natural variation it is perhaps necessary to consider both views, as the ground they cover are somewhat different. While the functional framework has the advantage of describing the underlying mode of action in a biologically understandable manner, the statistical framework assigns the degree of importance of the interaction term through the partitioning of variance. Crucially, partitioning in the statistical sense will depend on allele frequencies, and the qualitative result of the significance of the interaction terms is liable to change in different populations \citep{Greene2009}.

From a functional perspective, the highly integrated structure of molecular interactions that comprise cell, tissue and organism level systems would ostensibly suggest a strong predilection toward the manifestation of interactions between polymorphisms. However, the explicit modelling of epistasis in simulated biological systems does not necessarily precipitate large proportions of non-additive variation in a statistical framework. \citet{Keightley1989} demonstrated that for enzymatic pathways governing metabolic flux, where enzyme activity is under genetic control, significant dominance variance is manifested when frequencies are low, and the elevation of interaction terms requires multiple locus interactions and relatively large allelic differences. In essence, the additive variance can dominate even when non-additive functions are being specifically modelled. This conclusion was echoed in a more recent study \citep{Gjuvsland2007} that modelled a similar style of dynamic pathway system, but this time representing gene regulatory networks. Most simple models of multi-parameter regulatory networks create variance in gene expression that are mostly explained by additive variance, and only by introducing more exotic features such as positive feedback mechanisms does epistasis become a significant statistical component of the variation. 

Macro evolutionary modelling involving epistasis has a much longer history. Since first being incorporated by \citet{Wright1931} into evolutionary modelling it has remained prominent largely because of the enigmatic behaviour of genetic variation in life history traits, where typically genetic improvement cannot be made even with significant heritability estimates \citep{Hansen2004}. This approach forms the focus of chapter \ref{Results1}, and a more thorough overview of evolutionary modelling is detailed there. 

Although illuminating, it is often difficult to ascertain strong conclusions from these types of simulation studies simply because the biologically `correct' values for the underlying parameters are generally unknown. However, occasionally biological examples can be found for some of the abstract models that are postulated. One such case is the concept of `canalisation', or the buffering of variation. First coined by \citet{Waddington1942}, it initially postulated that, even in the face of environmental and genetic variation, developmental end-points tended to exhibit surprisingly small amounts of variation, particularly when compared to developmental mid-points. It is now common to refer to canalisation to mean phenotypic robustness in general, the tendency for phenotypes to be robust to genetic and environmental variation, and it is an important concept in terms of system networks and pathway redundancy \citep{Avery1992, Thomas1993}. Convincing examples exist from population based studies, for instance \citet{Bergman2003} demonstrated (in \emph{Drosophila} and \emph{Arabidposis}) that genetic variation in several pathways can accumulate benignly and it is not until the Hsp90 gene, an important hub in the network of protein interactions, is compromised that pleiotropic phenotypic variation is released. Similarly, \citet{Carlborg2006} showed that for crosses between chicken lines divergent for body weight the effects of five loci important for growth were only observed in a specific background of a sixth locus, which acted as a capacitor for the release of genetic variation. 

While at the population level these types of discoveries are relatively uncommon (perhaps because they are seldom searched for) and the genetic effects that are discovered could be argued to be largely additive \citep{Hill2008a}, at the molecular level perhaps the converse is true. Through mutation studies, particularly in haploid organisms but also common in \emph{Drosophila} and \emph{C. elegans} models, it is routine to encounter mutations that have dominant or recessive effects, and often these can be suppressed (reverted to wild type) by mutations at independent loci \citep{Wu1994, Hara1995}. Suppressor screens specifically search for such mutant strains, and the propensity for their success is high \citep{Madigan2009}. A similar case can be made for enhancer mutations, those that synergistically intensify the independent effects of each locus, and essentially these types of studies signpost the widespread redundancy within biological networks \citep{Brookfield1997}. But in the wider context of genetic variation, the significance of this is debatable. Firstly, the variants generated in mutation studies are unlikely to be representative of the genetic variation in natural populations. Secondly, with multiplicative effects like those manifested in many enhancer mutations, by rescaling the phenotype the mutations could lose the synergistic relationship, and be entirely explainable statistically without the need for an interaction term, so it is questionable as to whether this constitutes epistasis or not. Thirdly, the extent to which these types of interactions contribute to statistical epistasis is dependent on population frequencies, and there is a dearth of information regarding this.

Ultimately our understanding of the importance of epistasis today comes mainly from anecdotal evidence and indirect inference. But with the explosion in genomic data now being generated the opportunity to assay the architecture of genetic variation directly and to explicitly mine for epistasis is finally here. To summarise the discussion above, precedence for the search for genetic variance through an additive parameterisation exists, but it is likely overstated, and it has so far been difficult to gauge how much non-genetic variation actually exists. This thesis seeks to address this problem from three main aspects: exploration of the potential role of epistasis in the maintenance of genetic variance from an evolutionary perspective, development of computational tools for data mining, and development of statistical methods for improving the power of epistatic variant mapping. To summarise the objectives of the proceeding chapters:

\begin{itemize}
\item Chapter 2 explores the impact of epistasis on the additive variance in complex fitness related traits from an evolutionary perspective and concludes that there is precedence for the theory that epistasis is important in the maintenance of additive genetic variation. It goes on to explore the most 
statistically powerful GWAS parameterisation for the mapping of the types of genetic variation that is likely to be maintained under selection.
\item Chapter 3 discusses current data mining techniques for epistasis, and presents new parallel software that overcomes the computational burden for these types of analyses.
\item Chapters 4 and 5 address the statistical problems associated with the search for epistasis. Chapter 4 introduces the problems associated with the `curse of dimensionality' and presents empirical testing thresholds for standard epistatic GWASs. The statistical power for detection in GWAS is discussed in chapter 5, and a novel haplotype based method for detection and mapping of epistatic variants is presented.
\end{itemize}

